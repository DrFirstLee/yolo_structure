{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b4726e9-f6fa-4989-8718-bded6c32ccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import shutil\n",
    "import yaml\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18f836d0-c1ce-4aea-88e7-4a36710c1b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 80\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "model = YOLO(\"yolo11x.pt\")\n",
    "print(type(model.names),len(model.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b23c2c2-cd71-4b6c-831f-bca5502cda27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/6 /app/yolo_structure/test/Suwon_CH01_20200720_2000_MON_9m_NH_highway_TW5_sunny_FHD_005.png: 640x384 10 cars, 7 trucks, 7.6ms\n",
      "image 2/6 /app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png: 640x384 14 cars, 8 trucks, 7.4ms\n",
      "image 3/6 /app/yolo_structure/test/Suwon_CH02_20200721_2130_TUE_9m_NH_highway_TW5_sunny_FHD_005.png: 384x640 12 cars, 7.4ms\n",
      "image 4/6 /app/yolo_structure/test/Suwon_CH03_20200722_1400_WED_9m_NH_highway_OW5_rainy_FHD_034.png: 640x384 7 cars, 6 trucks, 7.5ms\n",
      "image 5/6 /app/yolo_structure/test/hh_dash.jpg: 384x640 1 person, 7 cars, 1 truck, 1 handbag, 7.4ms\n",
      "image 6/6 /app/yolo_structure/test/people_street.jpeg: 384x640 5 persons, 1 car, 1 motorcycle, 2 backpacks, 1 handbag, 1 cell phone, 7.2ms\n",
      "Speed: 0.7ms preprocess, 7.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict31\u001b[0m\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Labeled image saved at: /app/yolo_structure/llm_labeled_images/Suwon_CH01_20200720_2000_MON_9m_NH_highway_TW5_sunny_FHD_005.png\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Van.\n",
      "Ollama Result: VAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "TRUCK.\n",
      "Ollama Result: TRUCK\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "sedan.\n",
      "Ollama Result: SEDAN\n",
      "Labeled image saved at: /app/yolo_structure/llm_labeled_images/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png\n",
      "SUV.\n",
      "Ollama Result: SUV\n",
      "VAN\n",
      "Ollama Result: VAN\n",
      "SUV.\n",
      "Ollama Result: SUV\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "SUV.\n",
      "Ollama Result: SUV\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "SUV.\n",
      "Ollama Result: SUV\n",
      "Labeled image saved at: /app/yolo_structure/llm_labeled_images/Suwon_CH02_20200721_2130_TUE_9m_NH_highway_TW5_sunny_FHD_005.png\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "SUV.\n",
      "Ollama Result: SUV\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Truck.\n",
      "Ollama Result: TRUCK\n",
      "Truck.\n",
      "Ollama Result: TRUCK\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Labeled image saved at: /app/yolo_structure/llm_labeled_images/Suwon_CH03_20200722_1400_WED_9m_NH_highway_OW5_rainy_FHD_034.png\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Labeled image saved at: /app/yolo_structure/llm_labeled_images/hh_dash.jpg\n",
      "Sedan.\n",
      "Ollama Result: SEDAN\n",
      "Labeled image saved at: /app/yolo_structure/llm_labeled_images/people_street.jpeg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 현재 디렉토리와 테스트 경로 설정\n",
    "curr_dir = os.getcwd()\n",
    "test_loc = os.path.join(curr_dir, \"test\")\n",
    "crop_save_dir = os.path.join(curr_dir, \"crops\")\n",
    "output_dir = os.path.join(curr_dir, \"llm_labeled_images\")\n",
    "os.makedirs(crop_save_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ollama 모델 응답과 YOLO 클래스 이름 매핑\n",
    "ollama_to_yolo_mapping = {\n",
    "    \"SEDAN\": \"car\",\n",
    "    \"SUV\": \"car\",\n",
    "    \"VAN\": \"van\",\n",
    "    \"TRUCK\": \"truck\",\n",
    "    \"BUS\": \"bus\"\n",
    "}\n",
    "\n",
    "# YOLO 모델 로드\n",
    "model = YOLO(\"yolo11x.pt\")  # 첫 번째 모델 (기본 차량 감지용)\n",
    "\n",
    "# 첫 번째 모델로 감지 수행\n",
    "results = model.predict(source=test_loc, save=True)\n",
    "\n",
    "# Ollama 결과 반영 및 수정된 이미지 저장\n",
    "for result in results:\n",
    "    yolo_visualized_img = result.plot()\n",
    "    img_with_ollama = yolo_visualized_img.copy()\n",
    "    detections = result.boxes  # 감지된 바운딩 박스들\n",
    "    for i, box in enumerate(detections):\n",
    "        cls = int(box.cls)  # 기존 클래스 ID\n",
    "        original_class_name = result.names[cls]\n",
    "\n",
    "        # 'car' 클래스만 Ollama 호출\n",
    "        if original_class_name == \"car\":\n",
    "            # 바운딩 박스 좌표\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            # 'car' 영역을 잘라냄\n",
    "            car_crop = img_with_ollama[y1:y2, x1:x2]\n",
    "            crop_filename = os.path.join(crop_save_dir, f\"car_crop_{i:03}.jpg\")\n",
    "            cv2.imwrite(crop_filename, car_crop)\n",
    "            res = ollama.chat(\n",
    "            \tmodel= \"llama3.2-vision:11b\",\n",
    "            \tmessages=[\n",
    "            \t\t{\n",
    "            \t\t\t'role': 'user',\n",
    "            \t\t\t'content': \"tell me what is in this image. Answer only one in [SEDAN/SUV/VAN/TRUCK/BUS] answer in one word\",\n",
    "            \t\t\t'images': [crop_filename]\n",
    "            \t\t}\n",
    "            \t]\n",
    "            )\n",
    "            \n",
    "            print(res['message']['content'])\n",
    "\n",
    "            final_result = res['message']['content'].upper().replace(\".\",\"\").replace(\" \",\"\")\n",
    "            print(f\"Ollama Result: {final_result}\")\n",
    "            \n",
    "            # 바운딩 박스 그리기\n",
    "            color = (0, 255, 0)  # 초록색\n",
    "            thickness = 2\n",
    "            cv2.rectangle(img_with_ollama, (x1, y1), (x2, y2), color, thickness)\n",
    "            \n",
    "            # 이미지에 텍스트 추가 (final_result 표시)\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.6\n",
    "            color = (0, 255, 0)  # 초록색\n",
    "            thickness = 2\n",
    "            position = (x1 , y1 + 15)  # 바운딩 박스 위에 텍스트 표시\n",
    "\n",
    "            cv2.putText(img_with_ollama, final_result, position, font, font_scale, color, thickness)\n",
    "            \n",
    "    # 수정된 바운딩 박스를 포함한 이미지 저장\n",
    "    labeled_img_path = os.path.join(output_dir, os.path.basename(result.path))\n",
    "    cv2.imwrite(labeled_img_path, img_with_ollama)\n",
    "    print(f\"Labeled image saved at: {labeled_img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7a2c13-fc9d-4690-8dfa-35243f2dfd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([0.], device='cuda:0')\n",
       "conf: tensor([0.9343], device='cuda:0')\n",
       "data: tensor([[584.6871, 121.0828, 766.2991, 462.4198,   0.9343,   0.0000]], device='cuda:0')\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (467, 830)\n",
       "shape: torch.Size([1, 6])\n",
       "xywh: tensor([[675.4931, 291.7513, 181.6119, 341.3370]], device='cuda:0')\n",
       "xywhn: tensor([[0.8138, 0.6247, 0.2188, 0.7309]], device='cuda:0')\n",
       "xyxy: tensor([[584.6871, 121.0828, 766.2991, 462.4198]], device='cuda:0')\n",
       "xyxyn: tensor([[0.7044, 0.2593, 0.9233, 0.9902]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.boxes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee3769f-744a-4742-8a50-fbd9f9911719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/3 /app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png: 640x384 14 cars, 8 trucks, 7.5ms\n",
      "image 2/3 /app/yolo_structure/test/hh_dash.jpg: 384x640 1 person, 7 cars, 1 truck, 1 handbag, 7.4ms\n",
      "image 3/3 /app/yolo_structure/test/people_street.jpeg: 384x640 5 persons, 1 car, 1 motorcycle, 2 backpacks, 1 handbag, 1 cell phone, 7.2ms\n",
      "Speed: 0.6ms preprocess, 7.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_000.jpg\n",
      "/app/yolo_structure/crops/car_crop_000.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_001.jpg\n",
      "/app/yolo_structure/crops/car_crop_001.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_002.jpg\n",
      "/app/yolo_structure/crops/car_crop_002.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_003.jpg\n",
      "/app/yolo_structure/crops/car_crop_003.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_004.jpg\n",
      "/app/yolo_structure/crops/car_crop_004.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_006.jpg\n",
      "/app/yolo_structure/crops/car_crop_006.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_007.jpg\n",
      "/app/yolo_structure/crops/car_crop_007.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_010.jpg\n",
      "/app/yolo_structure/crops/car_crop_010.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_011.jpg\n",
      "/app/yolo_structure/crops/car_crop_011.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_012.jpg\n",
      "/app/yolo_structure/crops/car_crop_012.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_014.jpg\n",
      "/app/yolo_structure/crops/car_crop_014.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_018.jpg\n",
      "/app/yolo_structure/crops/car_crop_018.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_019.jpg\n",
      "/app/yolo_structure/crops/car_crop_019.jpg  =  SUV\n",
      "/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png car\n",
      "/app/yolo_structure/crops/car_crop_021.jpg\n",
      "/app/yolo_structure/crops/car_crop_021.jpg  =  SUV\n"
     ]
    }
   ],
   "source": [
    "# 현재 디렉토리와 테스트 경로 설정\n",
    "curr_dir = os.getcwd()\n",
    "test_loc = os.path.join(curr_dir, \"test\")\n",
    "crop_save_dir = os.path.join(curr_dir, \"crops\")\n",
    "\n",
    "# 모델 로드\n",
    "model = YOLO(\"yolo11x.pt\")  # 첫 번째 모델 (기본 차량 감지용)\n",
    "\n",
    "# 첫 번째 모델로 감지 수행\n",
    "results = model.predict(source=test_loc, save=True)\n",
    "\n",
    "# 감지된 결과에서 'car'만 추출\n",
    "for result in results:\n",
    "    img = result.orig_img  # 원본 이미지\n",
    "    detections = result.boxes  # 감지된 바운딩 박스들\n",
    "    \n",
    "    for i, box in enumerate(detections):\n",
    "        cls = int(box.cls)  # 클래스 ID\n",
    "        if result.names[cls] == \"car\":  # 'car' 클래스만 필터링\n",
    "            print(result.path, result.names[cls] )\n",
    "            # 바운딩 박스 좌표\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            \n",
    "\n",
    "            # 'car' 영역을 잘라냄\n",
    "            car_crop = img[y1:y2, x1:x2]\n",
    "            crop_filename = os.path.join(crop_save_dir, f\"car_crop_{i:03}.jpg\")\n",
    "            cv2.imwrite(crop_filename, car_crop)    \n",
    "            print(crop_filename)\n",
    "            # Ollama API 호출\n",
    "            res = ollama.chat(\n",
    "                model=\"llama3.2-vision:11b\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': \"tell me what is in this image. Answer only in [SEDAN/SUV/VAN/TRUCK/BUS]\",\n",
    "                        'images': [crop_filename] \n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            final_result = res['message']['content'].strip() \n",
    "            final_result = \"SUV\"\n",
    "            # 응답 출력\n",
    "            print(crop_filename , ' = ', final_result)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3063b41b-2d21-494a-b875-b1f78a919fb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "property 'cls' of 'Boxes' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m new_class_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m45\u001b[39m  \u001b[38;5;66;03m# 예: 새로운 클래스 ID\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mcls \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# 텐서를 수정 가능하도록 복제\u001b[39;00m\n\u001b[1;32m      3\u001b[0m result\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mcls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m44\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: property 'cls' of 'Boxes' object has no setter"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(curr_dir, \"llm_labeled_images\")\n",
    "os.makedirs(crop_save_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Ollama 모델 응답과 YOLO 클래스 이름 매핑\n",
    "ollama_to_yolo_mapping = {\n",
    "    \"SEDAN\": \"car\",\n",
    "    \"SUV\": \"car\",\n",
    "    \"VAN\": \"van\",\n",
    "    \"TRUCK\": \"truck\",\n",
    "    \"BUS\": \"bus\"\n",
    "}\n",
    "\n",
    "# YOLO 모델 로드\n",
    "model = YOLO(\"yolo11x.pt\")  # 첫 번째 모델 (기본 차량 감지용)\n",
    "\n",
    "# 첫 번째 모델로 감지 수행\n",
    "results = model.predict(source=test_loc, save=True)\n",
    "\n",
    "# Ollama 결과 반영 및 수정된 이미지 저장\n",
    "for result in results:\n",
    "    yolo_visualized_img = result.plot()\n",
    "    img_with_ollama = yolo_visualized_img.copy()\n",
    "    detections = result.boxes  # 감지된 바운딩 박스들\n",
    "    for i, box in enumerate(detections):\n",
    "        cls = int(box.cls)  # 기존 클래스 ID\n",
    "        original_class_name = result.names[cls]\n",
    "\n",
    "        # 'car' 클래스만 Ollama 호출\n",
    "        if original_class_name == \"car\":\n",
    "            # 바운딩 박스 좌표\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            # 'car' 영역을 잘라냄\n",
    "            car_crop = img_with_ollama[y1:y2, x1:x2]\n",
    "            crop_filename = os.path.join(crop_save_dir, f\"car_crop_{i:03}.jpg\")\n",
    "            cv2.imwrite(crop_filename, car_crop)\n",
    "            res = ollama.chat(\n",
    "            \tmodel= \"llama3.2-vision:11b\",\n",
    "            \tmessages=[\n",
    "            \t\t{\n",
    "            \t\t\t'role': 'user',\n",
    "            \t\t\t'content': \"tell me what is in this image. Answer only one in [SEDAN/SUV/VAN/TRUCK/BUS] answer in one word\",\n",
    "            \t\t\t'images': [crop_filename]\n",
    "            \t\t}\n",
    "            \t]\n",
    "            )\n",
    "            \n",
    "            print(res['message']['content'])\n",
    "\n",
    "            final_result = res['message']['content'].upper().replace(\".\",\"\").replace(\" \",\"\")\n",
    "            print(f\"Ollama Result: {final_result}\")\n",
    "            \n",
    "            # 바운딩 박스 그리기\n",
    "            color = (0, 255, 0)  # 초록색\n",
    "            thickness = 2\n",
    "            cv2.rectangle(img_with_ollama, (x1, y1), (x2, y2), color, thickness)\n",
    "            \n",
    "            # 이미지에 텍스트 추가 (final_result 표시)\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 0.6\n",
    "            color = (0, 255, 0)  # 초록색\n",
    "            thickness = 2\n",
    "            position = (x1 , y1 + 15)  # 바운딩 박스 위에 텍스트 표시\n",
    "\n",
    "            cv2.putText(img_with_ollama, final_result, position, font, font_scale, color, thickness)\n",
    "            \n",
    "    # 수정된 바운딩 박스를 포함한 이미지 저장\n",
    "    labeled_img_path = os.path.join(output_dir, os.path.basename(result.path))\n",
    "    cv2.imwrite(labeled_img_path, img_with_ollama)\n",
    "    print(f\"Labeled image saved at: {labeled_img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b0d19f7-fd6d-44cd-bdc1-5dc182e326f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.boxes[0].cls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecd16562-d61c-45f1-aef1-ace925bc6e61",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "property 'cls' of 'Boxes' object has no setter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result\u001b[38;5;241m.\u001b[39mboxes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcls \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: property 'cls' of 'Boxes' object has no setter"
     ]
    }
   ],
   "source": [
    "result.boxes[0].cls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18896bec-5884-402b-adc6-c110caa4d639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Boxes' object has no attribute 'box'. See valid attributes below.\n\n    A class for managing and manipulating detection boxes.\n\n    This class provides functionality for handling detection boxes, including their coordinates, confidence scores,\n    class labels, and optional tracking IDs. It supports various box formats and offers methods for easy manipulation\n    and conversion between different coordinate systems.\n\n    Attributes:\n        data (torch.Tensor | numpy.ndarray): The raw tensor containing detection boxes and associated data.\n        orig_shape (Tuple[int, int]): The original image dimensions (height, width).\n        is_track (bool): Indicates whether tracking IDs are included in the box data.\n        xyxy (torch.Tensor | numpy.ndarray): Boxes in [x1, y1, x2, y2] format.\n        conf (torch.Tensor | numpy.ndarray): Confidence scores for each box.\n        cls (torch.Tensor | numpy.ndarray): Class labels for each box.\n        id (torch.Tensor | numpy.ndarray): Tracking IDs for each box (if available).\n        xywh (torch.Tensor | numpy.ndarray): Boxes in [x, y, width, height] format.\n        xyxyn (torch.Tensor | numpy.ndarray): Normalized [x1, y1, x2, y2] boxes relative to orig_shape.\n        xywhn (torch.Tensor | numpy.ndarray): Normalized [x, y, width, height] boxes relative to orig_shape.\n\n    Methods:\n        cpu(): Returns a copy of the object with all tensors on CPU memory.\n        numpy(): Returns a copy of the object with all tensors as numpy arrays.\n        cuda(): Returns a copy of the object with all tensors on GPU memory.\n        to(*args, **kwargs): Returns a copy of the object with tensors on specified device and dtype.\n\n    Examples:\n        >>> import torch\n        >>> boxes_data = torch.tensor([[100, 50, 150, 100, 0.9, 0], [200, 150, 300, 250, 0.8, 1]])\n        >>> orig_shape = (480, 640)  # height, width\n        >>> boxes = Boxes(boxes_data, orig_shape)\n        >>> print(boxes.xyxy)\n        >>> print(boxes.conf)\n        >>> print(boxes.cls)\n        >>> print(boxes.xywhn)\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result\u001b[38;5;241m.\u001b[39mboxes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbox\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/ultralytics/utils/__init__.py:240\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Boxes' object has no attribute 'box'. See valid attributes below.\n\n    A class for managing and manipulating detection boxes.\n\n    This class provides functionality for handling detection boxes, including their coordinates, confidence scores,\n    class labels, and optional tracking IDs. It supports various box formats and offers methods for easy manipulation\n    and conversion between different coordinate systems.\n\n    Attributes:\n        data (torch.Tensor | numpy.ndarray): The raw tensor containing detection boxes and associated data.\n        orig_shape (Tuple[int, int]): The original image dimensions (height, width).\n        is_track (bool): Indicates whether tracking IDs are included in the box data.\n        xyxy (torch.Tensor | numpy.ndarray): Boxes in [x1, y1, x2, y2] format.\n        conf (torch.Tensor | numpy.ndarray): Confidence scores for each box.\n        cls (torch.Tensor | numpy.ndarray): Class labels for each box.\n        id (torch.Tensor | numpy.ndarray): Tracking IDs for each box (if available).\n        xywh (torch.Tensor | numpy.ndarray): Boxes in [x, y, width, height] format.\n        xyxyn (torch.Tensor | numpy.ndarray): Normalized [x1, y1, x2, y2] boxes relative to orig_shape.\n        xywhn (torch.Tensor | numpy.ndarray): Normalized [x, y, width, height] boxes relative to orig_shape.\n\n    Methods:\n        cpu(): Returns a copy of the object with all tensors on CPU memory.\n        numpy(): Returns a copy of the object with all tensors as numpy arrays.\n        cuda(): Returns a copy of the object with all tensors on GPU memory.\n        to(*args, **kwargs): Returns a copy of the object with tensors on specified device and dtype.\n\n    Examples:\n        >>> import torch\n        >>> boxes_data = torch.tensor([[100, 50, 150, 100, 0.9, 0], [200, 150, 300, 250, 0.8, 1]])\n        >>> orig_shape = (480, 640)  # height, width\n        >>> boxes = Boxes(boxes_data, orig_shape)\n        >>> print(boxes.xyxy)\n        >>> print(boxes.conf)\n        >>> print(boxes.cls)\n        >>> print(boxes.xywhn)\n    "
     ]
    }
   ],
   "source": [
    "result.boxes[0].box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7bdc62c-c54e-439f-8172-b20ed7ce2dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Results object with attributes:\n",
       "\n",
       "boxes: ultralytics.engine.results.Boxes object\n",
       "keypoints: None\n",
       "masks: None\n",
       "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       "obb: None\n",
       "orig_img: array([[[167, 168, 164],\n",
       "        [167, 168, 164],\n",
       "        [167, 168, 164],\n",
       "        ...,\n",
       "        [ 98,  94,  83],\n",
       "        [117, 113, 102],\n",
       "        [132, 128, 117]],\n",
       "\n",
       "       [[167, 168, 164],\n",
       "        [167, 168, 164],\n",
       "        [167, 168, 164],\n",
       "        ...,\n",
       "        [ 94,  90,  79],\n",
       "        [116, 112, 101],\n",
       "        [133, 129, 118]],\n",
       "\n",
       "       [[167, 168, 164],\n",
       "        [167, 168, 164],\n",
       "        [167, 168, 164],\n",
       "        ...,\n",
       "        [ 87,  83,  72],\n",
       "        [113, 109,  98],\n",
       "        [125, 121, 110]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[104, 106, 106],\n",
       "        [104, 106, 106],\n",
       "        [104, 106, 106],\n",
       "        ...,\n",
       "        [ 77,  76,  78],\n",
       "        [ 77,  76,  78],\n",
       "        [ 77,  76,  78]],\n",
       "\n",
       "       [[104, 106, 106],\n",
       "        [104, 106, 106],\n",
       "        [104, 106, 106],\n",
       "        ...,\n",
       "        [ 77,  76,  78],\n",
       "        [ 77,  76,  78],\n",
       "        [ 77,  76,  78]],\n",
       "\n",
       "       [[104, 106, 106],\n",
       "        [104, 106, 106],\n",
       "        [104, 106, 106],\n",
       "        ...,\n",
       "        [ 77,  76,  78],\n",
       "        [ 77,  76,  78],\n",
       "        [ 77,  76,  78]]], dtype=uint8)\n",
       "orig_shape: (1920, 1080)\n",
       "path: '/app/yolo_structure/test/Suwon_CH01_20200721_1500_TUE_9m_NH_highway_TW5_sunny_FHD_013.png'\n",
       "probs: None\n",
       "save_dir: 'runs/detect/predict7'\n",
       "speed: {'preprocess': 0.8447170257568359, 'inference': 7.509946823120117, 'postprocess': 0.4906654357910156}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c0405c0-f622-48bf-9c40-d0deae77915c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
